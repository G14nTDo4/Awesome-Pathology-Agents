# Awesome-Pathology-Agents

Awesome papers and datasets specifically focused on pathology.

- [Awesome-Pathology-Agents](#awesome-pathology-agents)
  - [Papers](#papers)
    - [Perception](#perception)
    - [Reasoning](#reasoning)
    - [Planning](#planning)
    - [Tool Use](#tool-use)
    - [Memory](#memory)
    - [Self-improvement](#self-improvement)
  - [Datasets](#datasets)
    - [Patch Level Visual Question Answering Datasets](#patch-level-visual-question-answering-datasets)
    - [Slide Level Visual Question Answering Datasets](#slide-level-visual-question-answering-datasets)
  - [Star History](#star-history)

## Papers

### Perception

* [PathAlign: A vision–language model for whole slide images in histopathology](https://arxiv.org/abs/2406.19578), MICCAI COMPAYL Workshop 2024.
* [WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_51), MICCAI 2024 oral, [![Star](https://img.shields.io/github/stars/cpystan/Wsi-Caption.svg?style=social&label=Star)](https://github.com/cpystan/Wsi-Caption).
* [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-Modal Context Interaction](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_18), MICCAI 2024, [![Star](https://img.shields.io/github/stars/dddavid4real/HistGen.svg?style=social&label=Star)](https://github.com/dddavid4real/HistGen).
* [PathChat: A multimodal generative AI copilot for human pathology](https://www.nature.com/articles/s41586-024-07618-3), Nature 2024, [![Star](https://img.shields.io/github/stars/fedshyvana/pathology_mllm_training.svg?style=social&label=Star)](https://github.com/fedshyvana/pathology_mllm_training).
* [A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis](https://arxiv.org/abs/2412.13126), arXiv 2024, [![Star](https://img.shields.io/github/stars/MAGIC-AI4Med/KEEP.svg?style=social&label=Star)](https://github.com/MAGIC-AI4Med/KEEP).
* [Virchow: A Million-Slide Digital Pathology Foundation Model](https://arxiv.org/abs/2309.07778), arxiv 2023.
* [Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology](https://arxiv.org/abs/2408.00738), arXiv 2025.
* [ALPaCA: Adapting Llama for Pathology Context Analysis to enable slide-level question answering](https://www.medrxiv.org/content/10.1101/2025.04.22.25326190v1), medrxiv 2025.
* [PathVG: A New Benchmark and Dataset for Pathology Visual Grounding](https://link.springer.com/chapter/10.1007/978-3-032-05169-1_44), MICCAI 2025, [![Star](https://img.shields.io/github/stars/ssecv/PathVG.svg?style=social&label=Star)](https://github.com/ssecv/PathVG).
* [MUSK: A vision–language foundation model for precision oncology](https://www.nature.com/articles/s41586-024-08378-w), Nature 2025, [![Star](https://img.shields.io/github/stars/lilab-stanford/MUSK.svg?style=social&label=Star)](https://github.com/lilab-stanford/MUSK).
* [TITAN: A multimodal whole-slide foundation model for pathology](https://www.nature.com/articles/s41591-025-03982-3), Nature Medicine 2025, [![Star](https://img.shields.io/github/stars/mahmoodlab/TITAN.svg?style=social&label=Star)](https://github.com/mahmoodlab/TITAN).
* [UNI: Towards a general-purpose foundation model for computational pathology](https://www.nature.com/articles/s41591-024-02857-3), Nature Medicine 2024, [![Star](https://img.shields.io/github/stars/mahmoodlab/UNI.svg?style=social&label=Star)](https://github.com/mahmoodlab/UNI).
* [CONCH: A visual-language foundation model for computational pathology](https://www.nature.com/articles/s41591-024-02856-4), Nature Medicine 2024, [![Star](https://img.shields.io/github/stars/mahmoodlab/CONCH.svg?style=social&label=Star)](https://github.com/mahmoodlab/CONCH).
* [PLIP: A visual–language foundation model for pathology image analysis using medical Twitter](https://www.nature.com/articles/s41591-023-02504-3), Nature Medicine 2023, [![Star](https://img.shields.io/github/stars/pharmai/plip.svg?style=social&label=Star)](https://github.com/pharmai/plip).
* [Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation](https://openaccess.thecvf.com/content/CVPR2025/html/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.html), CVPR 2025, [![Star](https://img.shields.io/github/stars/BasitAlawode/MR-PLIP.svg?style=social&label=Star)](https://github.com/BasitAlawode/MR-PLIP).




### Reasoning

* [SlideSeek: Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964), arXiv 2025.
* [SmartPath-R1: A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model](https://arxiv.org/abs/2507.17303), arXiv 2025.
* [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587), arXiv 2025, [![Star](https://img.shields.io/github/stars/zhihuanglab/Pathology-CoT.svg?style=social&label=Star)](https://github.com/zhihuanglab/Pathology-CoT).
* [TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots](https://www.researchsquare.com/article/rs-8098264/v1), arXiv 2025.
* [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/abs/2505.11404), AAAI 2026, [![Star](https://img.shields.io/github/stars/Wenchuan-Zhang/Patho-R1.svg?style=social&label=Star)](https://github.com/Wenchuan-Zhang/Patho-R1).
* [CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology](https://openaccess.thecvf.com/content/CVPR2025/html/Sun_CPath-Omni_A_Unified_Multimodal_Foundation_Model_for_Patch_and_Whole_CVPR_2025_paper.html), CVPR 2025, [![Star](https://img.shields.io/github/stars/PathFoundation/CPath-Omni.svg?style=social&label=Star)](https://github.com/PathFoundation/CPath-Omni).
* [SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SlideChat_A_Large_Vision-Language_Assistant_for_Whole-Slide_Pathology_Image_Understanding_CVPR_2025_paper.html), CVPR 2025, [![Star](https://img.shields.io/github/stars/uni-medical/SlideChat.svg?style=social&label=Star)](https://github.com/uni-medical/SlideChat).
* [PolyPath: Adapting a Large Multimodal Model for Multislide Pathology Report Generation](https://arxiv.org/abs/2502.10536), arxiv 2025.
* [PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology](https://ojs.aaai.org/index.php/AAAI/article/view/28308), AAAI 2024, [![Star](https://img.shields.io/github/stars/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.svg?style=social&label=Star)](https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology).
* [WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image](https://openaccess.thecvf.com/content/ICCV2025/html/Liang_WSI-LLaVA_A_Multimodal_Large_Language_Model_for_Whole_Slide_Image_ICCV_2025_paper.html), ICCV 2025, [![Star](https://img.shields.io/github/stars/XinhengLyu/WSI-LLaVA.svg?style=social&label=Star)](https://github.com/XinhengLyu/WSI-LLaVA).
* [HistoGPT: Generating dermatopathology reports from gigapixel whole slide images with HistoGPT](https://www.nature.com/articles/s41467-025-60014-x), Narture Communications 2025, [![Star](https://img.shields.io/github/stars/marrlab/HistoGPT.svg?style=social&label=Star)](https://github.com/marrlab/HistoGPT).
* [PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue](https://arxiv.org/abs/2506.13063), arXiv 2025.
* [Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos](https://www.nature.com/articles/s41586-024-07618-3), CVPR 2024, [![Star](https://img.shields.io/github/stars/aldraus/quilt-llava.svg?style=social&label=Star)](https://github.com/aldraus/quilt-llava).

### Planning

* [PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology](https://arxiv.org/abs/2502.08916), ICCV 2025.
* [PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning](https://arxiv.org/abs/2511.17052), arXiv 2025, [![Star](https://img.shields.io/github/stars/G14nTDo4/PathAgent.svg?style=social&label=Star)](https://github.com/G14nTDo4/PathAgent).
* [CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists’ Diagnostic Logic](https://arxiv.org/pdf/2505.20510), NeurIPS 2025.
* [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](https://arxiv.org/abs/2511.16635), arXiv 2025.


### Tool Use

* [WSI-Agents: A Collaborative Multi-agent System for Multi-modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680), MICCAI 2025, [![Star](https://img.shields.io/github/stars/XinhengLyu/WSI-Agents.svg?style=social&label=Star)](https://github.com/XinhengLyu/WSI-Agents).
* [UnPuzzle: A Unified Framework for Pathology Image Analysis](https://arxiv.org/abs/2503.03152), arXiv 2025, [![Star](https://img.shields.io/github/stars/Puzzle-Logic/UnPuzzle.svg?style=social&label=Star)](https://github.com/Puzzle-Logic/UnPuzzle).

### Memory

* [Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning](https://arxiv.org/abs/2508.02258), AAAI 2026, [![Star](https://img.shields.io/github/stars/Wenchuan-Zhang/Patho-AgenticRAG.svg?style=social&label=Star)](https://github.com/Wenchuan-Zhang/Patho-AgenticRAG).
* [TissueLab: A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279), arXiv 2025, [![Star](https://img.shields.io/github/stars/zhihuanglab/TissueLab.svg?style=social&label=Star)](https://github.com/zhihuanglab/TissueLab).

### Self-improvement

* [TissueLab: A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279), arXiv 2025, [![Star](https://img.shields.io/github/stars/zhihuanglab/TissueLab.svg?style=social&label=Star)](https://github.com/zhihuanglab/TissueLab).


## Datasets

***Latest Papers (after 2023)***
[ Dataset ] Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers, CVPR 2024.
[ Dataset ] MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies, 2024.
  A novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of GPT-4 and text-to-image models to generate detailed scripts and corresponding visuals.

### Patch Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

### Slide Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

## Benchmarks

* [PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology](https://link.springer.com/chapter/10.1007/978-3-031-73033-7_4), ECCV 2024, Benchmark.
* [PathBench: Advancing the Benchmark of Large Multimodal Models for Pathology Image Understanding at Patch and Whole Slide Level](https://ieeexplore.ieee.org/abstract/document/11062674/), TMI 2025, Benchmark.
* [PathoGaze1.0: Eye-Tracking, Mouse Tracking, Stimulus Tracking, and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653), arXiv 2025, Clinical hardware data collection.
* [PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration](https://openreview.net/forum?id=rFpZnn11gj), ICLR 2025 oral, Benchmark.
* [Quilt-1M: One Million Image-Text Pairs for Histopathology](https://proceedings.neurips.cc/paper_files/paper/2023/file/775ec578876fa6812c062644964b9870-Paper-Datasets_and_Benchmarks.pdf), NeurIPS 2023, Benchmark.
* [Hest-1k: A dataset for spatial transcriptomics and histology image analysis](https://proceedings.neurips.cc/paper_files/paper/2024/file/60a899cc31f763be0bde781a75e04458-Paper-Datasets_and_Benchmarks_Track.pdf), NeurIPS 2024, Benchmark.
* [STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics](https://proceedings.neurips.cc/paper_files/paper/2024/file/3ef2b740cb22dcce67c20989cb3d3fce-Paper-Datasets_and_Benchmarks_Track.pdf), NeurIPS 2024, Benchmark, [![Star](https://img.shields.io/github/stars/JiawenChenn/STimage-1K4M.svg?style=social&label=Star)](https://github.com/JiawenChenn/STimage-1K4M).

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=G14nTDo4/PathAgent&type=date&legend=top-left)](https://www.star-history.com/#G14nTDo4/PathAgent&type=date&legend=top-left)
