# Awesome-Pathology-Agents

Awesome papers and datasets specifically focused on pathology.

- [Awesome-Pathology-Agents](#awesome-pathology-agents)
  - [Papers](#papers)
    - [Perception](#perception)
    - [Reasoning](#reasoning)
    - [Planning](#planning)
    - [Tool Use](#tool-use)
    - [Memory](#memory)
    - [Self-improvement](#self-improvement)
  - [Datasets](#datasets)
    - [Patch Level Visual Question Answering Datasets](#patch-level-visual-question-answering-datasets)
    - [Slide Level Visual Question Answering Datasets](#slide-level-visual-question-answering-datasets)
  - [Star History](#star-history)

## Papers

### Perception

* [PathAlign: A vision–language model for whole slide images in histopathology](https://arxiv.org/abs/2406.19578), MICCAI COMPAYL Workshop 2024.
* [WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_51), MICCAI 2024 oral.
* [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-Modal Context Interaction](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_18), MICCAI 2024.
* [PathChat: A multimodal generative AI copilot for human pathology](https://www.nature.com/articles/s41586-024-07618-3), Nature 2024.
* [A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis](https://arxiv.org/abs/2412.13126), arXiv 2024.
* [Virchow: A Million-Slide Digital Pathology Foundation Model](https://arxiv.org/abs/2309.07778), arxiv 2023.
* [Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology](https://arxiv.org/abs/2408.00738), arXiv 2025.
* [ALPaCA: Adapting Llama for Pathology Context Analysis to enable slide-level question answering](https://www.medrxiv.org/content/10.1101/2025.04.22.25326190v1), medrxiv 2025.
* [PathVG: A New Benchmark and Dataset for Pathology Visual Grounding](https://link.springer.com/chapter/10.1007/978-3-032-05169-1_44), MICCAI 2025.
* [MUSK: A vision–language foundation model for precision oncology](https://www.nature.com/articles/s41586-024-08378-w), Nature 2025.
* [TITAN: A multimodal whole-slide foundation model for pathology](https://www.nature.com/articles/s41591-025-03982-3), Nature Medicine 2025.
* [UNI: Towards a general-purpose foundation model for computational pathology](https://www.nature.com/articles/s41591-024-02857-3), Nature Medicine 2024.
* [CONCH: A visual-language foundation model for computational pathology](https://www.nature.com/articles/s41591-024-02856-4), Nature Medicine 2024.
* [PLIP: A visual–language foundation model for pathology image analysis using medical Twitter](https://www.nature.com/articles/s41591-023-02504-3), Nature Medicine 2023.
* [Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation
](https://openaccess.thecvf.com/content/CVPR2025/html/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.html), CVPR 2025, [![Star](https://img.shields.io/github/stars/BasitAlawode/MR-PLIP.svg?style=social&label=Star)](https://github.com/BasitAlawode/MR-PLIP).




### Reasoning

* [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964), arXiv 2025.
* [SmartPath-R1: A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model](https://arxiv.org/abs/2507.17303), arXiv 2025.
* [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587), arXiv 2025.
* [TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots](https://www.researchsquare.com/article/rs-8098264/v1), arXiv 2025.
* [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/abs/2505.11404), AAAI 2026.
* [CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology](https://openaccess.thecvf.com/content/CVPR2025/html/Sun_CPath-Omni_A_Unified_Multimodal_Foundation_Model_for_Patch_and_Whole_CVPR_2025_paper.html), CVPR 2025.
* [SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SlideChat_A_Large_Vision-Language_Assistant_for_Whole-Slide_Pathology_Image_Understanding_CVPR_2025_paper.html), CVPR 2025.
* [PolyPath: Adapting a Large Multimodal Model for Multislide Pathology Report Generation](https://arxiv.org/abs/2502.10536), arxiv 2025.
* [PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology](https://ojs.aaai.org/index.php/AAAI/article/view/28308), AAAI 2024.
* [WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image](https://openaccess.thecvf.com/content/ICCV2025/html/Liang_WSI-LLaVA_A_Multimodal_Large_Language_Model_for_Whole_Slide_Image_ICCV_2025_paper.html), ICCV 2025.
* [HistoGPT: Generating dermatopathology reports from gigapixel whole slide images with HistoGPT](https://www.nature.com/articles/s41467-025-60014-x), Narture Communications 2025.
* [PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue](https://arxiv.org/abs/2506.13063), arXiv 2025.
* [Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos](https://www.nature.com/articles/s41586-024-07618-3), CVPR 2024.

### Planning

* [PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology](https://arxiv.org/abs/2502.08916), ICCV 2025.
* [PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning](https://arxiv.org/abs/2511.17052), arXiv 2025.
* [CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists’ Diagnostic Logic](https://arxiv.org/pdf/2505.20510), NeurIPS 2025.
* [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](https://arxiv.org/abs/2511.16635), arXiv 2025.


### Tool Use

* [WSI-Agents: A Collaborative Multi-agent System for Multi-modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680), MICCAI 2025.
* [UnPuzzle: A Unified Framework for Pathology Image Analysis](https://arxiv.org/abs/2503.03152), arXiv 2025.

### Memory

* [Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning](https://arxiv.org/abs/2508.02258), AAAI 2026.
* [TissueLab: A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279), arXiv 2025.

### Self-improvement

* [TissueLab: A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279), arXiv 2025.

### Other

* [PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology](https://link.springer.com/chapter/10.1007/978-3-031-73033-7_4), ECCV 2024, Benchmark.
* [PathBench: Advancing the Benchmark of Large Multimodal Models for Pathology Image Understanding at Patch and Whole Slide Level](https://ieeexplore.ieee.org/abstract/document/11062674/), TMI 2025, Benchmark.
* [PathoGaze1.0: Eye-Tracking, Mouse Tracking, Stimulus Tracking, and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653), arXiv 2025, Clinical hardware data collection.
* [PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration](https://openreview.net/forum?id=rFpZnn11gj), ICLR 2025 oral, Benchmark.
* [Quilt-1M: One Million Image-Text Pairs for Histopathology](https://proceedings.neurips.cc/paper_files/paper/2023/file/775ec578876fa6812c062644964b9870-Paper-Datasets_and_Benchmarks.pdf), NeurIPS 2023, Benchmark.


## Datasets

***Latest Papers (after 2023)***
[ Dataset ] Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers, CVPR 2024.
[ Dataset ] MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies, 2024.
  A novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of GPT-4 and text-to-image models to generate detailed scripts and corresponding visuals.

### Patch Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

### Slide Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

## Benchmarks

***Latest Papers (after 2023)***
* [**TemporalBench**: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models],arXiv[![arXiv](https://img.shields.io/badge/arXiv-2410.10818-b31b1b.svg?style=plastic)](https://www.arxiv.org/abs/2410.10818)
* [**VideoHallucer**: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models](https://arxiv.org/abs/2406.16338), arXiv[![arXiv](https://img.shields.io/badge/arXiv-2406.16338-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2406.16338)
* Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,2024
* CinePile: A Long Video Question Answering Dataset and Benchmark,2024
* TempCompass: Do Video LLMs Really Understand Videos? 2024.
* MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, CVPR 2024.
* How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs, 2024
* MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding [Arxiv](https://arxiv.org/abs/2406.04264)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=G14nTDo4/PathAgent&type=date&legend=top-left)](https://www.star-history.com/#G14nTDo4/PathAgent&type=date&legend=top-left)
