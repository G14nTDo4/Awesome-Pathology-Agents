# Awesome-Pathology-Agents

Awesome papers and datasets specifically focused on pathology.

- [Awesome-Pathology-Agents](#awesome-pathology-agents)
  - [Patch Level Agents](#patch-level-agents)
  - [Slide Level Agents](#slide-level-agents)
  - [Datasets](#datasets)
    - [Patch Level Visual Question Answering Datasets](#patch-level-visual-question-answering-datasets)
    - [Slide Level Visual Question Answering Datasets](#slide-level-visual-question-answering-datasets)
  - [Star History](#star-history)

## Patch Level Agents

* PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning, arXiv 2025.
* PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning, arXiv 2025.

## Slide Level Agents

* PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning, arXiv 2025.
* PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning, arXiv 2025.

## Datasets

***Latest Papers (after 2023)***
[ Dataset ] Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers, CVPR 2024.
[ Dataset ] MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies, 2024.
  A novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of GPT-4 and text-to-image models to generate detailed scripts and corresponding visuals.

### Patch Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

### Slide Level Visual Question Answering Datasets

| Dataset              | Annotation                                      | Source               | Number             | Duration | Tasks                                   | link                                                                                                                                                | Date Released |
| -------------------- | ----------------------------------------------- | -------------------- | ------------------ | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| EgoLife | timestamps + captions | Daily Life | 6 | 44.3h | Understanding & Reasoning | [Data](https://huggingface.co/collections/lmms-lab/egolife-67c04574c2a9b64ab312c342), [Proj Page](https://egolife-ai.github.io/), [Paper](https://arxiv.org/abs/2503.03803) | 2025
| ActivityNet 1.3      | timestamps + action                             | Youtube              | 20k                | -        | Action Localization                     |                                                                                                                                                     |               |
| ActivityNet Captions | timestamps + captions                           | Youtube              | 20k                | -        | Dense captioning, video grounding       |                                                                                                                                                     |               |
| THUMOS               | timestamps + action                             | -                    | -                  | -        | Action Localization                     |                                                                                                                                                     |               |
| YouCook2             | timestamps + captions                           | Cooking Videos       | -                  | -        | Dense captioning                        |                                                                                                                                                     |               |
| MovieNet             | timestamps + captions + place/action/style tags | Movies               | 1.1k               | >2h      | movie understanding                     | [MovieNet](https://movienet.site/)                                                                                                                     | 2020          |

## Benchmarks

***Latest Papers (after 2023)***
* [**TemporalBench**: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models],arXiv[![arXiv](https://img.shields.io/badge/arXiv-2410.10818-b31b1b.svg?style=plastic)](https://www.arxiv.org/abs/2410.10818)
* [**VideoHallucer**: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models](https://arxiv.org/abs/2406.16338), arXiv[![arXiv](https://img.shields.io/badge/arXiv-2406.16338-b31b1b.svg?style=plastic)](https://arxiv.org/abs/2406.16338)
* Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,2024
* CinePile: A Long Video Question Answering Dataset and Benchmark,2024
* TempCompass: Do Video LLMs Really Understand Videos? 2024.
* MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, CVPR 2024.
* How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs, 2024
* MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding [Arxiv](https://arxiv.org/abs/2406.04264)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=G14nTDo4/PathAgent&type=date&legend=top-left)](https://www.star-history.com/#G14nTDo4/PathAgent&type=date&legend=top-left)
